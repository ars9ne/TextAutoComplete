{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84a77e98",
   "metadata": {},
   "source": [
    "Этап 0-1. Подготовка данных и окружения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc2d776d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seed(seed=1):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(1)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81095e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 1278086\n",
      "val  : 159761\n",
      "test : 159761\n",
      "\n",
      "--- train examples ---\n",
      "non- fail. ali found cappachino mix. now, to mail rin a cappachino. this might not fit in the mail box.\n",
      "i am at a cool party. talking about twitter. but work tomorrow... mirka says hi. maria too.\n",
      "procastinated by doing up a new playlist for tcc. now off to nalinas for teh 18th birthday bash. 2forever\n",
      "essay limit 2000 words. my count: 1999.\n",
      "is excited\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "train_path = \"./data/splits/train.txt\"\n",
    "val_path   = \"./data/splits/val.txt\"\n",
    "test_path  = \"./data/splits/test.txt\"\n",
    "\n",
    "def count_lines(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        return sum(1 for _ in f)\n",
    "\n",
    "def head(path, n=5):\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for i, line in zip(range(n), f):\n",
    "            print(line.strip())\n",
    "\n",
    "print(\"train:\", count_lines(train_path))\n",
    "print(\"val  :\", count_lines(val_path))\n",
    "print(\"test :\", count_lines(test_path))\n",
    "print(\"\\n--- train examples ---\")\n",
    "head(train_path, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de14f3fb",
   "metadata": {},
   "source": [
    "Vocab + токенизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ab54f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 20000\n",
      "pad/bos/eos/unk: 0 2 3 1\n",
      "encoded: [2, 4, 60, 44, 5, 874, 204, 70, 3]\n",
      "decoded: i am going to learn something new\n"
     ]
    }
   ],
   "source": [
    "from src.tokenizer_utils import build_vocab, encode, decode\n",
    "MAX_VOCAB_SIZE = 20000\n",
    "MIN_FREQ = 3\n",
    "token2id, id2token = build_vocab(train_path, max_vocab_size=MAX_VOCAB_SIZE, min_freq=MIN_FREQ)\n",
    "vocab_size = len(token2id)\n",
    "\n",
    "pad_id = token2id[\"<pad>\"]\n",
    "bos_id = token2id[\"<bos>\"]\n",
    "eos_id = token2id[\"<eos>\"]\n",
    "unk_id = token2id[\"<unk>\"]\n",
    "\n",
    "print(\"vocab_size:\", vocab_size)\n",
    "print(\"pad/bos/eos/unk:\", pad_id, bos_id, eos_id, unk_id)\n",
    "\n",
    "sample = \"i am going to learn something new\"\n",
    "ids = encode(sample, token2id, add_bos=True, add_eos=True)\n",
    "print(\"encoded:\", ids[:20])\n",
    "print(\"decoded:\", decode(ids, id2token))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa6dc19",
   "metadata": {},
   "source": [
    "Dataset + DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8424e9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: torch.Size([128, 31]) torch.int64\n",
      "target: torch.Size([128, 31]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from src.next_token_dataset import NextTokenDataset, collate_fn\n",
    "\n",
    "MAX_LEN = 64\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_ds = NextTokenDataset(train_path, token2id, max_len=MAX_LEN)\n",
    "val_ds   = NextTokenDataset(val_path, token2id, max_len=MAX_LEN)\n",
    "test_ds  = NextTokenDataset(test_path, token2id, max_len=MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          collate_fn=lambda b: collate_fn(b, pad_id))\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          collate_fn=lambda b: collate_fn(b, pad_id))\n",
    "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          collate_fn=lambda b: collate_fn(b, pad_id))\n",
    "\n",
    "x, y = next(iter(train_loader))\n",
    "print(\"input:\", x.shape, x.dtype)\n",
    "print(\"target:\", y.shape, y.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4498e1f9",
   "metadata": {},
   "source": [
    "Этап 2. LSTM модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e746920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: 5272096\n"
     ]
    }
   ],
   "source": [
    "from src.lstm_model import LSTMNextToken\n",
    "\n",
    "EMB_DIM = 128\n",
    "HIDDEN_DIM = 128\n",
    "\n",
    "model = LSTMNextToken(vocab_size=vocab_size, emb_dim=EMB_DIM, hidden_dim=HIDDEN_DIM, pad_id=pad_id).to(DEVICE)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"params: {n_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de8bb4a",
   "metadata": {},
   "source": [
    "Этап 3. Обучение + сохранение + Rouge метрика для LSTM + Тестирование LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a0eabe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9986/9986 [08:21<00:00, 19.93it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 5.3333 | Val Loss: 5.0652 | Val r-1: 0.0657 | Val r-2: 0.0088\n",
      "-------Пример----------\n",
      "PROMPT: exams\n",
      "TARGET: here\n",
      "PRED  : \n",
      "-------Пример----------\n",
      "PROMPT: headache. not\n",
      "TARGET: nice.\n",
      "PRED  : sure\n",
      "-------Пример----------\n",
      "PROMPT: has an upset\n",
      "TARGET: \n",
      "PRED  : for\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9986/9986 [04:05<00:00, 40.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 4.9890 | Val Loss: 4.9502 | Val r-1: 0.0742 | Val r-2: 0.0115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9986/9986 [03:53<00:00, 42.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 4.8928 | Val Loss: 4.8951 | Val r-1: 0.0773 | Val r-2: 0.0106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9986/9986 [03:53<00:00, 42.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss: 4.8358 | Val Loss: 4.8625 | Val r-1: 0.0803 | Val r-2: 0.0119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9986/9986 [03:59<00:00, 41.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss: 4.7964 | Val Loss: 4.8390 | Val r-1: 0.0804 | Val r-2: 0.0114\n",
      "-------Пример----------\n",
      "PROMPT: exams\n",
      "TARGET: here\n",
      "PRED  : \n",
      "-------Пример----------\n",
      "PROMPT: headache. not\n",
      "TARGET: nice.\n",
      "PRED  : a\n",
      "-------Пример----------\n",
      "PROMPT: has an upset\n",
      "TARGET: \n",
      "PRED  : \n",
      "Best val Rouge2: 0.011892443165574178\n"
     ]
    }
   ],
   "source": [
    "from src.lstm_train import train_model\n",
    "\n",
    "n_epoch = 5\n",
    "lr = 1e-3\n",
    "save_path = \"./models/lstm_best.pth\"\n",
    "\n",
    "train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    token2id,\n",
    "    id2token,\n",
    "    n_epochs=n_epoch,\n",
    "    lr=lr,\n",
    "    save_path=save_path,\n",
    "    rouge_batches=32,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae8d052c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM VAL : ROUGE-1=0.0766 ROUGE-2=0.0110\n",
      "LSTM TEST: ROUGE-1=0.0773 ROUGE-2=0.0123\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from src.eval_lstm import evaluate_rouge\n",
    "from src.lstm_model import LSTMNextToken\n",
    "\n",
    "best = LSTMNextToken(vocab_size=vocab_size, emb_dim=EMB_DIM, hidden_dim=HIDDEN_DIM, pad_id=pad_id).to(DEVICE)\n",
    "best.load_state_dict(torch.load(save_path, map_location=DEVICE))\n",
    "\n",
    "val_r1, val_r2 = evaluate_rouge(best, val_loader, token2id, id2token, max_batches=100, q=0.25)\n",
    "test_r1, test_r2 = evaluate_rouge(best, test_loader, token2id, id2token, max_batches=100, q=0.25)\n",
    "\n",
    "print(f\"LSTM VAL : ROUGE-1={val_r1:.4f} ROUGE-2={val_r2:.4f}\")\n",
    "print(f\"LSTM TEST: ROUGE-1={test_r1:.4f} ROUGE-2={test_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "28d6e409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from src.lstm_model import LSTMNextToken\n",
    "from src.tokenizer_utils import encode, decode\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMNextToken(vocab_size=vocab_size, emb_dim=EMB_DIM, hidden_dim=HIDDEN_DIM, pad_id=pad_id).to(DEVICE)\n",
    "model.load_state_dict(torch.load(\"./models/lstm_best.pth\", map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "prompt_text = \"watch youtube\"\n",
    "prompt_ids = encode(prompt_text, token2id, add_bos=True, add_eos=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd3873a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.lstm_model import LSTMNextToken\n",
    "model = LSTMNextToken(vocab_size=vocab_size, emb_dim=EMB_DIM, hidden_dim=HIDDEN_DIM, pad_id=pad_id).to(DEVICE)\n",
    "model.load_state_dict(torch.load(\"./models/lstm_best.pth\", map_location=DEVICE))\n",
    "model.eval()\n",
    "prompt = \"watch youtube\"\n",
    "generated_full = model.evaluate(prompt, token2id, id2token, num_tokens=15)\n",
    "\n",
    "print(\"PROMPT:\", prompt)\n",
    "print(\"GEN   :\", generated_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "88e01467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "promt: watch youtube\n",
      "generated №1: watch youtube\n",
      "generated №2: watch youtube videos\n",
      "generated №3: watch youtube so on the now i am sick and hungry\n",
      "generated №4: watch youtube and a good with\n",
      "generated №5: watch youtube from the show the new trailer show so im not\n"
     ]
    }
   ],
   "source": [
    "def sample_next_id(next_logits, temperature=1.0, top_k=50):\n",
    "    next_logits = next_logits / max(temperature, 1e-8)\n",
    "    if top_k is None:\n",
    "        return int(torch.argmax(next_logits).item())\n",
    "\n",
    "    k = min(top_k, next_logits.numel())\n",
    "    vals, idx = torch.topk(next_logits, k)\n",
    "    probs = F.softmax(vals, dim=-1)\n",
    "    choice = torch.multinomial(probs, num_samples=1).item()\n",
    "    return int(idx[choice].item())\n",
    "\n",
    "def generate_lstm(model, prefix_ids, max_new_tokens=20, temperature=1.0, top_k=50):\n",
    "    ids = list(prefix_ids)\n",
    "    for _ in range(max_new_tokens):\n",
    "        x = torch.tensor([ids], dtype=torch.long, device=DEVICE)\n",
    "        with torch.no_grad():\n",
    "            logits = model(x)\n",
    "            next_logits = logits[0, -1]\n",
    "        next_id = sample_next_id(next_logits, temperature=temperature, top_k=top_k)\n",
    "        ids.append(next_id)\n",
    "        if next_id == eos_id:\n",
    "            break\n",
    "    return ids\n",
    "\n",
    "import random\n",
    "outs = []\n",
    "for i in range(5): #5 разных генераций\n",
    "    torch.manual_seed(random.randint(1, 1000))\n",
    "    out_ids = generate_lstm(model, prompt_ids, max_new_tokens=10, temperature=1, top_k=50)\n",
    "    outs.append(out_ids)\n",
    "\n",
    "print(\"promt:\", prompt_text)\n",
    "for i, out_ids in enumerate(outs, 1):\n",
    "    print(f\"generated №{i}:\", decode(out_ids, id2token))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbb01ae",
   "metadata": {},
   "source": [
    "Этап 4. distilgpt2 + ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "afd5ecca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\Text-Autocomplete\\text-autocomplete-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Prompt: is picking up her new passport today... watch ou\n",
      "Target: world - im back\n",
      "Prediction: , i am going in the middle of the day\n",
      "ROUGE-1: 0.0000 | ROUGE-2: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Prompt: broddddddddddy we love you your unreal lt;3\n",
      "Target: danni and sash\n",
      "Prediction: e1e2e3e3e3\n",
      "ROUGE-1: 0.0000 | ROUGE-2: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Prompt: just finished fallout 3 - great game! felt cheated at the end though. i had\n",
      "Target: enough rad-away to make it\n",
      "Prediction: tried to save 2 - i think it was a\n",
      "ROUGE-1: 0.2667 | ROUGE-2: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Prompt: wow, just called a bit dissapointed! been a customer for 9 years now.\n",
      "Target: maybe not for much longer\n",
      "Prediction: The experience and the quality. The food was good\n",
      "ROUGE-1: 0.0000 | ROUGE-2: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Prompt: i wonder if cubicle 7 is still coming out with the doctor who rpg. its\n",
      "Target: not mentioned on their forums anymore.\n",
      "Prediction: the most popular thing today.\n",
      "\n",
      "The problem with the\n",
      "ROUGE-1: 0.0000 | ROUGE-2: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilgpt2 VAL: n=139 ROUGE-1=0.0501 ROUGE-2=0.0026\n"
     ]
    }
   ],
   "source": [
    "from src.eval_transformer_pipeline import evaluate_transformer\n",
    "\n",
    "gpt2_val_r1, gpt2_val_r2, gpt2_val_n = evaluate_transformer(val_path, device=-1, max_samples=200, num_print=5)\n",
    "print(f\"distilgpt2 VAL: n={gpt2_val_n} ROUGE-1={gpt2_val_r1:.4f} ROUGE-2={gpt2_val_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4600cc9f",
   "metadata": {},
   "source": [
    "Этап 5. Сравнение + выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "a00b8898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>split</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>val</td>\n",
       "      <td>0.076582</td>\n",
       "      <td>0.011046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>test</td>\n",
       "      <td>0.077290</td>\n",
       "      <td>0.012269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>distilgpt2</td>\n",
       "      <td>val</td>\n",
       "      <td>0.050075</td>\n",
       "      <td>0.002638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        model split    rouge1    rouge2\n",
       "0        LSTM   val  0.076582  0.011046\n",
       "1        LSTM  test  0.077290  0.012269\n",
       "2  distilgpt2   val  0.050075  0.002638"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = pd.DataFrame([\n",
    "    {\"model\": \"LSTM\", \"split\": \"val\",  \"rouge1\": float(val_r1),      \"rouge2\": float(val_r2)},\n",
    "    {\"model\": \"LSTM\", \"split\": \"test\", \"rouge1\": float(test_r1),     \"rouge2\": float(test_r2)},\n",
    "    {\"model\": \"distilgpt2\",         \"split\": \"val\",  \"rouge1\": float(gpt2_val_r1), \"rouge2\": float(gpt2_val_r2)},\n",
    "])\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd9254b",
   "metadata": {},
   "source": [
    "Выводы:\n",
    "\n",
    "1. Обученная LSTM модель для автодополнения текста в сравнении с предобученным disitlgpt2 показывает более высокие значения ROUGE-1/ROUGE-2 (F1 score) на валидации и тесте, тем самым чаще воиспрозводила те же слова/биграммы, что и в реальном продолжение текста.\n",
    "2. Более низкий ROUGE у distilgpt2 можно объяснить тем, что модель не была предобучена на выбранном датасете, поэтому её продолжения чаще не совпадают с реальными окончаниям слов/биграмм.\n",
    "3. С учётом требования запускать модель на мобильных устройствах рекомендованно использовать LSTM как более легковесную и быструю модель. distilgpt2 стоит использовать при высоких требованиях к качеству и связности текста генерации."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-autocomplete-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
